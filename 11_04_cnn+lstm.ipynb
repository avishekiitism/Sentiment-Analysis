{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old work related to the paper ' Convolutional Neural Networks for Sentence Classification ' \n",
    "- https://arxiv.org/pdf/1408.5882.pdf\n",
    "\n",
    "It train a simple (CNN) with one layer of convolution on top of word vectors obtained from an unsupervised neural language model. These vectors were trained by Mikolov etal. (2013) on 100 billion words of Google News, and are publicly available.1 We initially keep the word vectors static and learn only the other parameters of the model. Despite little tuning of hyperparameters, this simple model achieves excellent results on multiple benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is new work we add on the old one and our contributions?\n",
    "\n",
    "Instead of unsing only the CNN to make the Sentence Classification we will use CNN in as well as LSTM to generate a combination model of them( CNN-LSTM ) and ( LSTM-CNN )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T12:26:20.461833Z",
     "iopub.status.busy": "2021-12-11T12:26:20.461583Z",
     "iopub.status.idle": "2021-12-11T12:26:20.469693Z",
     "shell.execute_reply": "2021-12-11T12:26:20.468354Z",
     "shell.execute_reply.started": "2021-12-11T12:26:20.461804Z"
    }
   },
   "source": [
    "# CNNs\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are networks initially created for image-related tasks that can learn to capture specific features regardless of locality.\n",
    "\n",
    "For a more concrete example of that, imagine we use CNNs to distinguish pictures of Cars vs. pictures of Dogs. Since CNNs learn to capture features regardless of where these might be, the CNN will learn that cars have wheels, and every time it sees a wheel, regardless of where it is on the picture, that feature will activate.\n",
    "\n",
    "In our particular case, it could capture a negative phrase such as \"don't like\" regardless of where it happens in the tweet.\n",
    "\n",
    "*     I don't like watching those types of films\n",
    "*     That's the one thing I really don't like.\n",
    "*     I saw the movie, and I don't like how it ended.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# LSTMs\n",
    "\n",
    "Long-Term Short Term Memory (LSTMs) are a type of network that has a memory that \"remembers\" previous data from the input and makes decisions based on that knowledge. These networks are more directly suited for written data inputs, since each word in a sentence has meaning based on the surrounding words (previous and upcoming words).\n",
    "\n",
    "In our particular case, it is possible that an LSTM could allow us to capture changing sentiment in a tweet. For example, a sentence such as: At first I loved it, but then I ended up hating it. has words with conflicting sentiments that would end-up confusing a simple Feed-Forward network. The LSTM, on the other hand, could learn that sentiments expressed towards the end of a sentence mean more than those expressed at the start.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM Model\n",
    "\n",
    "The first model I tried was the CNN-LSTM Model. Our CNN-LSTM model combination consists of an initial convolution layer which will receive word embeddings as input. Its output will then be pooled to a smaller dimension which is then fed into an LSTM layer. The intuition behind this model is that the convolution layer will extract local features and the LSTM layer will then be able to use the ordering of said features to learn about the input’s text ordering. In practice, this model is not as powerful as our other LSTM-CNN model proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-CNN Model\n",
    "\n",
    "Our CNN-LSTM model consists of an initial LSTM layer which will receive word embeddings for each token in the tweet as inputs. The intuition is that its output tokens will store information not only of the initial token, but also any previous tokens; In other words, the LSTM layer is generating a new encoding for the original input. The output of the LSTM layer is then fed into a convolution layer which we expect will extract local features. Finally the convolution layer’s output will be pooled to a smaller dimension and ultimately outputted as either a positive or negative label.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-11T13:47:26.757730Z",
     "iopub.status.idle": "2024-04-11T13:47:26.758034Z",
     "shell.execute_reply": "2024-04-11T13:47:26.757892Z",
     "shell.execute_reply.started": "2024-04-11T13:47:26.757871Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Avishek\n",
      "[nltk_data]     kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:45.821671Z",
     "iopub.status.busy": "2024-04-11T13:47:45.820967Z",
     "iopub.status.idle": "2024-04-11T13:47:52.648351Z",
     "shell.execute_reply": "2024-04-11T13:47:52.647649Z",
     "shell.execute_reply.started": "2024-04-11T13:47:45.821633Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "import os\n",
    "from IPython.display import Image\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#import transformers\n",
    "#import tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:52.650796Z",
     "iopub.status.busy": "2024-04-11T13:47:52.650547Z",
     "iopub.status.idle": "2024-04-11T13:47:54.034347Z",
     "shell.execute_reply": "2024-04-11T13:47:54.033713Z",
     "shell.execute_reply.started": "2024-04-11T13:47:52.650766Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Corona_NLP_train.csv', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.035568Z",
     "iopub.status.busy": "2024-04-11T13:47:54.035356Z",
     "iopub.status.idle": "2024-04-11T13:47:54.052127Z",
     "shell.execute_reply": "2024-04-11T13:47:54.051406Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.035541Z"
    }
   },
   "outputs": [],
   "source": [
    "data['sentiment']=data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.054447Z",
     "iopub.status.busy": "2024-04-11T13:47:54.054085Z",
     "iopub.status.idle": "2024-04-11T13:47:54.059594Z",
     "shell.execute_reply": "2024-04-11T13:47:54.058872Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.054408Z"
    }
   },
   "outputs": [],
   "source": [
    "data['review']=data['OriginalTweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "            sentiment                                             review  \n",
       "0             Neutral  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...  \n",
       "1            Positive  advice Talk to your neighbours family to excha...  \n",
       "2            Positive  Coronavirus Australia: Woolworths to give elde...  \n",
       "3            Positive  My food stock is not the only one which is emp...  \n",
       "4  Extremely Negative  Me, ready to go at supermarket during the #COV...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.061546Z",
     "iopub.status.busy": "2024-04-11T13:47:54.060892Z",
     "iopub.status.idle": "2024-04-11T13:47:54.096924Z",
     "shell.execute_reply": "2024-04-11T13:47:54.096266Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.061515Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41157 entries, 0 to 41156\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   UserName       41157 non-null  int64 \n",
      " 1   ScreenName     41157 non-null  int64 \n",
      " 2   Location       32567 non-null  object\n",
      " 3   TweetAt        41157 non-null  object\n",
      " 4   OriginalTweet  41157 non-null  object\n",
      " 5   Sentiment      41157 non-null  object\n",
      " 6   sentiment      41157 non-null  object\n",
      " 7   review         41157 non-null  object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 2.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.098133Z",
     "iopub.status.busy": "2024-04-11T13:47:54.097921Z",
     "iopub.status.idle": "2024-04-11T13:47:54.193330Z",
     "shell.execute_reply": "2024-04-11T13:47:54.192599Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.098108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>41157.000000</td>\n",
       "      <td>41157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24377.000000</td>\n",
       "      <td>69329.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11881.146851</td>\n",
       "      <td>11881.146851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3799.000000</td>\n",
       "      <td>48751.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14088.000000</td>\n",
       "      <td>59040.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24377.000000</td>\n",
       "      <td>69329.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>34666.000000</td>\n",
       "      <td>79618.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>44955.000000</td>\n",
       "      <td>89907.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           UserName    ScreenName\n",
       "count  41157.000000  41157.000000\n",
       "mean   24377.000000  69329.000000\n",
       "std    11881.146851  11881.146851\n",
       "min     3799.000000  48751.000000\n",
       "25%    14088.000000  59040.000000\n",
       "50%    24377.000000  69329.000000\n",
       "75%    34666.000000  79618.000000\n",
       "max    44955.000000  89907.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.194771Z",
     "iopub.status.busy": "2024-04-11T13:47:54.194469Z",
     "iopub.status.idle": "2024-04-11T13:47:54.207086Z",
     "shell.execute_reply": "2024-04-11T13:47:54.206392Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.194732Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive              11422\n",
       "Negative               9917\n",
       "Neutral                7713\n",
       "Extremely Positive     6624\n",
       "Extremely Negative     5481\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.208787Z",
     "iopub.status.busy": "2024-04-11T13:47:54.208398Z",
     "iopub.status.idle": "2024-04-11T13:47:54.231562Z",
     "shell.execute_reply": "2024-04-11T13:47:54.230837Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.208754Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UserName            0\n",
       "ScreenName          0\n",
       "Location         8590\n",
       "TweetAt             0\n",
       "OriginalTweet       0\n",
       "Sentiment           0\n",
       "sentiment           0\n",
       "review              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.233249Z",
     "iopub.status.busy": "2024-04-11T13:47:54.232527Z",
     "iopub.status.idle": "2024-04-11T13:47:54.408032Z",
     "shell.execute_reply": "2024-04-11T13:47:54.407266Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.233209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.410578Z",
     "iopub.status.busy": "2024-04-11T13:47:54.410351Z",
     "iopub.status.idle": "2024-04-11T13:47:54.566061Z",
     "shell.execute_reply": "2024-04-11T13:47:54.565442Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.410550Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.567542Z",
     "iopub.status.busy": "2024-04-11T13:47:54.567257Z",
     "iopub.status.idle": "2024-04-11T13:47:54.573502Z",
     "shell.execute_reply": "2024-04-11T13:47:54.572797Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.567504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.574945Z",
     "iopub.status.busy": "2024-04-11T13:47:54.574632Z",
     "iopub.status.idle": "2024-04-11T13:47:54.588064Z",
     "shell.execute_reply": "2024-04-11T13:47:54.587274Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.574911Z"
    }
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.589721Z",
     "iopub.status.busy": "2024-04-11T13:47:54.589489Z",
     "iopub.status.idle": "2024-04-11T13:47:54.604393Z",
     "shell.execute_reply": "2024-04-11T13:47:54.603728Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.589678Z"
    }
   },
   "outputs": [],
   "source": [
    "mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
    "           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n",
    "           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n",
    "           \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "           \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "           \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "           \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n",
    "           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "           \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "           \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "           \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
    "           \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n",
    "           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n",
    "           \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
    "           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n",
    "           \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "           \"what'll\": \"what will\", \"what'll've\": \"what will have\",\"what're\": \"what are\",  \n",
    "           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
    "           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
    "           \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "           \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "           \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "           \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "           \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "           \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.605579Z",
     "iopub.status.busy": "2024-04-11T13:47:54.605382Z",
     "iopub.status.idle": "2024-04-11T13:47:54.745734Z",
     "shell.execute_reply": "2024-04-11T13:47:54.744932Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.605554Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Avishek\n",
      "[nltk_data]     kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#function to clean data\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "def clean_text(text,lemmatize = True):\n",
    "    soup = BeautifulSoup(text, \"html.parser\") #remove html tags\n",
    "    text = soup.get_text()\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")]) #expanding chatwords and contracts clearing contractions\n",
    "    emoji_clean= re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_clean.sub(r'',text)\n",
    "    text = re.sub(r'\\.(?=\\S)', '. ',text) #add space after full stop\n",
    "    text = re.sub(r'http\\S+', '', text) #remove urls\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #remove punctuation\n",
    "    #tokens = re.split('\\W+', text) #create tokens\n",
    "    if lemmatize:\n",
    "        text = \" \".join([wl.lemmatize(word) for word in text.split() if word not in stop and word.isalpha()]) #lemmatize\n",
    "    else:\n",
    "        text = \" \".join([word for word in text.split() if word not in stop and word.isalpha()]) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:47:54.747088Z",
     "iopub.status.busy": "2024-04-11T13:47:54.746865Z",
     "iopub.status.idle": "2024-04-11T13:49:30.216872Z",
     "shell.execute_reply": "2024-04-11T13:49:30.216196Z",
     "shell.execute_reply.started": "2024-04-11T13:47:54.747061Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\bs4\\__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data['review']=data['review'].apply(clean_text,lemmatize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert score to sentiment\n",
    "def to_sentiment(rating):\n",
    "    \n",
    "    if(rating=='Neutral'):\n",
    "        return 2\n",
    "    elif rating=='Positive':\n",
    "        return 3\n",
    "    elif rating=='Extremely Positive':\n",
    "        return 4\n",
    "    elif rating=='Negative':\n",
    "        return 1\n",
    "    elif rating=='Extremely Negative':\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:49:30.218082Z",
     "iopub.status.busy": "2024-04-11T13:49:30.217883Z",
     "iopub.status.idle": "2024-04-11T13:49:30.262852Z",
     "shell.execute_reply": "2024-04-11T13:49:30.262091Z",
     "shell.execute_reply.started": "2024-04-11T13:49:30.218057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>menyrbie philgahan chrisitv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "      <td>advice talk neighbour family exchange phone nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "      <td>coronavirus australia woolworth give elderly d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>3</td>\n",
       "      <td>food stock one empty please panic enough food ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "      <td>0</td>\n",
       "      <td>ready go supermarket outbreak paranoid food st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \\\n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral   \n",
       "1  advice Talk to your neighbours family to excha...            Positive   \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive   \n",
       "3  My food stock is not the only one which is emp...            Positive   \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative   \n",
       "\n",
       "   sentiment                                             review  \n",
       "0          2                        menyrbie philgahan chrisitv  \n",
       "1          3  advice talk neighbour family exchange phone nu...  \n",
       "2          3  coronavirus australia woolworth give elderly d...  \n",
       "3          3  food stock one empty please panic enough food ...  \n",
       "4          0  ready go supermarket outbreak paranoid food st...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Apply to the dataset \n",
    "data['sentiment'] = data.Sentiment.apply(to_sentiment)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spliting the training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:49:30.264332Z",
     "iopub.status.busy": "2024-04-11T13:49:30.264056Z",
     "iopub.status.idle": "2024-04-11T13:49:30.283054Z",
     "shell.execute_reply": "2024-04-11T13:49:30.282349Z",
     "shell.execute_reply.started": "2024-04-11T13:49:30.264293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32925,) (32925,)\n",
      "(8232,) (8232,)\n"
     ]
    }
   ],
   "source": [
    "#splitting into train and test\n",
    "train, test= train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "#train dataset\n",
    "Xtrain, ytrain = train['review'], train['sentiment']\n",
    "\n",
    "#test dataset\n",
    "Xtest, ytest = test['review'], test['sentiment']\n",
    "\n",
    "print(Xtrain.shape,ytrain.shape)\n",
    "print(Xtest.shape,ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:49:58.061278Z",
     "iopub.status.busy": "2024-04-11T13:49:58.060722Z",
     "iopub.status.idle": "2024-04-11T13:50:12.792294Z",
     "shell.execute_reply": "2024-04-11T13:50:12.791628Z",
     "shell.execute_reply.started": "2024-04-11T13:49:58.061242Z"
    }
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "Xtrain_vect= vect.fit_transform(Xtrain)\n",
    "Xtest_vect = vect.transform(Xtest)\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer() \n",
    "Xtrain_count = count_vect.fit_transform(Xtrain)\n",
    "Xtest_count = count_vect.transform(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:50:12.794503Z",
     "iopub.status.busy": "2024-04-11T13:50:12.793980Z",
     "iopub.status.idle": "2024-04-11T13:50:17.475266Z",
     "shell.execute_reply": "2024-04-11T13:50:17.474466Z",
     "shell.execute_reply.started": "2024-04-11T13:50:12.794461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary of the dataset is :  47031\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 10000\n",
    "tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE,oov_token=\"<oov>\")\n",
    "tokenizer.fit_on_texts(Xtrain)\n",
    "word_index = tokenizer.word_index\n",
    "#print(word_index)\n",
    "V = len(word_index)\n",
    "print(\"Vocabulary of the dataset is : \",V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:50:17.476514Z",
     "iopub.status.busy": "2024-04-11T13:50:17.476285Z",
     "iopub.status.idle": "2024-04-11T13:50:21.892231Z",
     "shell.execute_reply": "2024-04-11T13:50:21.891425Z",
     "shell.execute_reply.started": "2024-04-11T13:50:17.476484Z"
    }
   },
   "outputs": [],
   "source": [
    "##create sequences of reviews\n",
    "seq_train = tokenizer.texts_to_sequences(Xtrain)\n",
    "seq_test =  tokenizer.texts_to_sequences(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:50:21.894447Z",
     "iopub.status.busy": "2024-04-11T13:50:21.894217Z",
     "iopub.status.idle": "2024-04-11T13:50:21.907642Z",
     "shell.execute_reply": "2024-04-11T13:50:21.906876Z",
     "shell.execute_reply.started": "2024-04-11T13:50:21.894417Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of sequence in the list: 39\n"
     ]
    }
   ],
   "source": [
    "#choice of maximum length of sequences\n",
    "seq_len_list = [len(i) for i in seq_train + seq_test]\n",
    "\n",
    "#if we take the direct maximum then\n",
    "max_len=max(seq_len_list)\n",
    "print('Maximum length of sequence in the list: {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:50:21.909023Z",
     "iopub.status.busy": "2024-04-11T13:50:21.908759Z",
     "iopub.status.idle": "2024-04-11T13:50:21.939207Z",
     "shell.execute_reply": "2024-04-11T13:50:21.938430Z",
     "shell.execute_reply.started": "2024-04-11T13:50:21.908981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of the sequence when considering data only two standard deviations from average: 29\n"
     ]
    }
   ],
   "source": [
    "# when setting the maximum length of sequence, variability around the average is used.\n",
    "max_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)\n",
    "max_seq_len = int(max_seq_len)\n",
    "print('Maximum length of the sequence when considering data only two standard deviations from average: {}'.format(max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:50:21.940998Z",
     "iopub.status.busy": "2024-04-11T13:50:21.940364Z",
     "iopub.status.idle": "2024-04-11T13:50:21.960051Z",
     "shell.execute_reply": "2024-04-11T13:50:21.959294Z",
     "shell.execute_reply.started": "2024-04-11T13:50:21.940957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The above calculated number coveres approximately 98.17 % of data\n"
     ]
    }
   ],
   "source": [
    "perc_covered = np.sum(np.array(seq_len_list) < max_seq_len) / len(seq_len_list)*100\n",
    "print('The above calculated number coveres approximately {} % of data'.format(np.round(perc_covered,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:50:21.961182Z",
     "iopub.status.busy": "2024-04-11T13:50:21.960972Z",
     "iopub.status.idle": "2024-04-11T13:50:23.343627Z",
     "shell.execute_reply": "2024-04-11T13:50:23.342881Z",
     "shell.execute_reply.started": "2024-04-11T13:50:21.961157Z"
    }
   },
   "outputs": [],
   "source": [
    "#create padded sequences\n",
    "pad_train=pad_sequences(seq_train,truncating = 'post', padding = 'pre',maxlen=max_seq_len)\n",
    "pad_test=pad_sequences(seq_test,truncating = 'post', padding = 'pre',maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:50:23.345048Z",
     "iopub.status.busy": "2024-04-11T13:50:23.344780Z",
     "iopub.status.idle": "2024-04-11T13:50:23.372513Z",
     "shell.execute_reply": "2024-04-11T13:50:23.371890Z",
     "shell.execute_reply.started": "2024-04-11T13:50:23.345013Z"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting training set for validation purposes\n",
    "Xtrain,Xval,ytrain,yval=train_test_split(pad_train,ytrain,\n",
    "                                             test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:50:23.373836Z",
     "iopub.status.busy": "2024-04-11T13:50:23.373597Z",
     "iopub.status.idle": "2024-04-11T13:50:23.384481Z",
     "shell.execute_reply": "2024-04-11T13:50:23.383763Z",
     "shell.execute_reply.started": "2024-04-11T13:50:23.373807Z"
    }
   },
   "outputs": [],
   "source": [
    "def lstm_model(Xtrain,Xval,ytrain,yval,V,D,maxlen,epochs):\n",
    "\n",
    "    print(\"----Building the model----\")\n",
    "    i = Input(shape=(maxlen,))\n",
    "    x = Embedding(V + 1, D,input_length = maxlen)(i)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Conv1D(32,5,activation = 'relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "    x = Bidirectional(LSTM(128,return_sequences=True))(x)\n",
    "    x = LSTM(64)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    model = Model(i, x)\n",
    "    model.summary()\n",
    "\n",
    "    #Training the LSTM\n",
    "    print(\"----Training the network----\")\n",
    "    model.compile(optimizer= Adam(0.0005),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    r = model.fit(Xtrain,ytrain, \n",
    "                  validation_data = (Xval,yval), \n",
    "                  epochs = epochs, \n",
    "                  verbose = 2,\n",
    "                  batch_size = 32)\n",
    "                  #callbacks = callbacks\n",
    "    print(\"Train score:\", model.evaluate(Xtrain,ytrain))\n",
    "    print(\"Validation score:\", model.evaluate(Xval,yval))\n",
    "    n_epochs = len(r.history['loss'])\n",
    "    \n",
    "    return r,model,n_epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-11T13:50:23.386763Z",
     "iopub.status.busy": "2024-04-11T13:50:23.386522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Building the model----\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 29)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 29, 64)            3010048   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 29, 64)           256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 29, 64)            0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 25, 32)            10272     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 25, 32)            0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 12, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 12, 256)          164864    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                82176     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,267,681\n",
      "Trainable params: 3,267,553\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "----Training the network----\n",
      "Epoch 1/5\n",
      "824/824 - 68s - loss: 0.0040 - accuracy: 0.9990 - val_loss: 2.3305e-05 - val_accuracy: 1.0000 - 68s/epoch - 82ms/step\n",
      "Epoch 2/5\n",
      "824/824 - 55s - loss: 4.7043e-05 - accuracy: 1.0000 - val_loss: 9.5946e-06 - val_accuracy: 1.0000 - 55s/epoch - 67ms/step\n",
      "Epoch 3/5\n",
      "824/824 - 59s - loss: 2.4986e-05 - accuracy: 1.0000 - val_loss: 5.0183e-06 - val_accuracy: 1.0000 - 59s/epoch - 72ms/step\n",
      "Epoch 4/5\n",
      "824/824 - 58s - loss: 1.5599e-05 - accuracy: 1.0000 - val_loss: 2.7812e-06 - val_accuracy: 1.0000 - 58s/epoch - 71ms/step\n",
      "Epoch 5/5\n",
      "824/824 - 59s - loss: 9.6416e-06 - accuracy: 1.0000 - val_loss: 1.6706e-06 - val_accuracy: 1.0000 - 59s/epoch - 71ms/step\n",
      "824/824 [==============================] - 10s 12ms/step - loss: 1.6703e-06 - accuracy: 1.0000\n",
      "Train score: [1.6702942957635969e-06, 1.0]\n",
      "206/206 [==============================] - 2s 10ms/step - loss: 1.6706e-06 - accuracy: 1.0000\n",
      "Validation score: [1.6706457017789944e-06, 1.0]\n"
     ]
    }
   ],
   "source": [
    "D = 64 #embedding dims\n",
    "epochs = 20\n",
    "r,model,n_epochs = lstm_model(Xtrain,Xval,ytrain,yval,V,D,max_seq_len,epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearningCurve(history,epochs):\n",
    "    \n",
    "    epochRange = range(1,epochs+1)\n",
    "    fig , ax = plt.subplots(1,2,figsize = (10,5))\n",
    "  \n",
    "    ax[0].plot(epochRange,history.history['accuracy'],label = 'Training Accuracy')\n",
    "    ax[0].plot(epochRange,history.history['val_accuracy'],label = 'Validation Accuracy')\n",
    "    ax[0].set_title('Training and Validation accuracy')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(epochRange,history.history['loss'],label = 'Training Loss')\n",
    "    ax[1].plot(epochRange,history.history['val_loss'],label = 'Validation Loss')\n",
    "    ax[1].set_title('Training and Validation loss')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Loss')\n",
    "    ax[1].legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLearningCurve(r,n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluate Model Performance on Test set\")\n",
    "result = model.evaluate(pad_test,ytest)\n",
    "print(dict(zip(model.metrics_names, result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate predictions for the test dataset\n",
    "ypred = model.predict(pad_test)\n",
    "ypred = ypred>0.5\n",
    "#Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.ylabel('True label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:06.467111Z",
     "iopub.status.busy": "2021-12-11T11:59:06.466639Z",
     "iopub.status.idle": "2021-12-11T11:59:06.483359Z",
     "shell.execute_reply": "2021-12-11T11:59:06.481137Z",
     "shell.execute_reply.started": "2021-12-11T11:59:06.467056Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, MaxPool1D, Dropout, SimpleRNN, LSTM\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:06.485214Z",
     "iopub.status.busy": "2021-12-11T11:59:06.484626Z",
     "iopub.status.idle": "2021-12-11T11:59:06.526446Z",
     "shell.execute_reply": "2021-12-11T11:59:06.525579Z",
     "shell.execute_reply.started": "2021-12-11T11:59:06.485176Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Apply to the dataset \n",
    "df['sentiment'] = df.Sentiment.apply(to_sentiment)\n",
    "text = df.review.tolist()\n",
    "label = df.sentiment.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:06.53328Z",
     "iopub.status.busy": "2021-12-11T11:59:06.531184Z",
     "iopub.status.idle": "2021-12-11T11:59:06.601688Z",
     "shell.execute_reply": "2021-12-11T11:59:06.60082Z",
     "shell.execute_reply.started": "2021-12-11T11:59:06.53324Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:06.60857Z",
     "iopub.status.busy": "2021-12-11T11:59:06.606486Z",
     "iopub.status.idle": "2021-12-11T11:59:06.61495Z",
     "shell.execute_reply": "2021-12-11T11:59:06.614278Z",
     "shell.execute_reply.started": "2021-12-11T11:59:06.608527Z"
    }
   },
   "outputs": [],
   "source": [
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:06.62192Z",
     "iopub.status.busy": "2021-12-11T11:59:06.619542Z",
     "iopub.status.idle": "2021-12-11T11:59:09.694868Z",
     "shell.execute_reply": "2021-12-11T11:59:09.694124Z",
     "shell.execute_reply.started": "2021-12-11T11:59:06.621881Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_clean = []\n",
    "X_test_clean = []\n",
    "clean = re.compile(r'<[^>]+>')\n",
    "for i, test in enumerate(X_train):\n",
    "    tmp_text = test.lower()\n",
    "    tmp_text = tmp_text.replace('\\n', '')\n",
    "    tmp_text = clean.sub('', tmp_text)\n",
    "    tmp_text = tmp_text.translate(translator)\n",
    "    X_train_clean.append(tmp_text)\n",
    "\n",
    "for i, test in enumerate(X_test):\n",
    "    tmp_text = test.lower()\n",
    "    tmp_text = tmp_text.replace('\\n', '')\n",
    "    tmp_text = clean.sub('', tmp_text)\n",
    "    tmp_text = tmp_text.translate(translator)\n",
    "    X_test_clean.append(tmp_text)\n",
    "\n",
    "X_train_clean = np.array(X_train_clean)\n",
    "X_test_clean = np.array(X_test_clean)\n",
    "\n",
    "X_train = X_train_clean\n",
    "X_test = X_test_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:09.696376Z",
     "iopub.status.busy": "2021-12-11T11:59:09.696121Z",
     "iopub.status.idle": "2021-12-11T11:59:29.360623Z",
     "shell.execute_reply": "2021-12-11T11:59:29.359788Z",
     "shell.execute_reply.started": "2021-12-11T11:59:09.696343Z"
    }
   },
   "outputs": [],
   "source": [
    "top_words = 40000\n",
    "tokenizer = Tokenizer(num_words=top_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:29.362733Z",
     "iopub.status.busy": "2021-12-11T11:59:29.361739Z",
     "iopub.status.idle": "2021-12-11T11:59:30.692294Z",
     "shell.execute_reply": "2021-12-11T11:59:30.691537Z",
     "shell.execute_reply.started": "2021-12-11T11:59:29.362659Z"
    }
   },
   "outputs": [],
   "source": [
    "max_words = 100\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:30.694233Z",
     "iopub.status.busy": "2021-12-11T11:59:30.69396Z",
     "iopub.status.idle": "2021-12-11T11:59:30.708581Z",
     "shell.execute_reply": "2021-12-11T11:59:30.707137Z",
     "shell.execute_reply.started": "2021-12-11T11:59:30.694198Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:30.710445Z",
     "iopub.status.busy": "2021-12-11T11:59:30.709988Z",
     "iopub.status.idle": "2021-12-11T11:59:30.721174Z",
     "shell.execute_reply": "2021-12-11T11:59:30.720391Z",
     "shell.execute_reply.started": "2021-12-11T11:59:30.710376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32925, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:30.724158Z",
     "iopub.status.busy": "2021-12-11T11:59:30.723925Z",
     "iopub.status.idle": "2021-12-11T11:59:30.79377Z",
     "shell.execute_reply": "2021-12-11T11:59:30.793082Z",
     "shell.execute_reply.started": "2021-12-11T11:59:30.724122Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000,32, input_length=100))\n",
    "model.add(Conv1D(256, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:30.796405Z",
     "iopub.status.busy": "2021-12-11T11:59:30.794886Z",
     "iopub.status.idle": "2021-12-11T11:59:30.807136Z",
     "shell.execute_reply": "2021-12-11T11:59:30.806331Z",
     "shell.execute_reply.started": "2021-12-11T11:59:30.796368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 32)           640000    \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 100, 256)          24832     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 50, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 50, 256)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 50, 128)           98432     \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 25, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 25, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3200)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 250)               800250    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,563,765\n",
      "Trainable params: 1,563,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T11:59:30.808781Z",
     "iopub.status.busy": "2021-12-11T11:59:30.808516Z",
     "iopub.status.idle": "2021-12-11T12:00:09.031808Z",
     "shell.execute_reply": "2021-12-11T12:00:09.031015Z",
     "shell.execute_reply.started": "2021-12-11T11:59:30.808748Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=20, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T12:00:09.033485Z",
     "iopub.status.busy": "2021-12-11T12:00:09.033198Z",
     "iopub.status.idle": "2021-12-11T12:00:09.782482Z",
     "shell.execute_reply": "2021-12-11T12:00:09.781737Z",
     "shell.execute_reply.started": "2021-12-11T12:00:09.033447Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Evaluate Model Performance on Test set\")\n",
    "result = model.evaluate(X_test,y_test)\n",
    "print(dict(zip(model.metrics_names, result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- LSTM-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T12:00:09.784023Z",
     "iopub.status.busy": "2021-12-11T12:00:09.783795Z",
     "iopub.status.idle": "2021-12-11T12:00:10.257747Z",
     "shell.execute_reply": "2021-12-11T12:00:10.257078Z",
     "shell.execute_reply.started": "2021-12-11T12:00:09.783993Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000,32, input_length=100))\n",
    "model.add(Conv1D(256, 3, activation='relu', input_shape=(178, 1), padding='same'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 3, activation='relu', padding='same'))\n",
    "model.add(MaxPool1D(2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T12:00:10.25925Z",
     "iopub.status.busy": "2021-12-11T12:00:10.258996Z",
     "iopub.status.idle": "2021-12-11T12:00:10.272107Z",
     "shell.execute_reply": "2021-12-11T12:00:10.271351Z",
     "shell.execute_reply.started": "2021-12-11T12:00:10.259217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 32)           640000    \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 100, 256)          24832     \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 50, 256)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50, 256)           0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 50, 128)           98432     \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 25, 128)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 25, 128)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 25, 64)            49408     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 32)                12416     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 250)               8250      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 833,589\n",
      "Trainable params: 833,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T12:00:10.273899Z",
     "iopub.status.busy": "2021-12-11T12:00:10.273624Z",
     "iopub.status.idle": "2021-12-11T12:01:13.208587Z",
     "shell.execute_reply": "2021-12-11T12:01:13.207897Z",
     "shell.execute_reply.started": "2021-12-11T12:00:10.27386Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=20, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-11T12:01:13.214395Z",
     "iopub.status.busy": "2021-12-11T12:01:13.212388Z",
     "iopub.status.idle": "2021-12-11T12:01:14.529929Z",
     "shell.execute_reply": "2021-12-11T12:01:14.529209Z",
     "shell.execute_reply.started": "2021-12-11T12:01:13.214356Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Evaluate Model Performance on Test set\")\n",
    "result = model.evaluate(X_test,y_test)\n",
    "print(dict(zip(model.metrics_names, result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "- http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/\n",
    "- https://arxiv.org/pdf/1408.5882.pdf\n",
    "- https://github.com/pytorch/ignite/blob/master/examples/notebooks/TextCNN.ipynb\n",
    "- https://www.kaggle.com/raghav2002sharma/sentiment-classifier-with-cnn-bi-lstm\n",
    "- https://www.kaggle.com/ashrafkhan94/imdb-review-comparison-using-cnn-lstm-bert\n",
    "- https://www.kaggle.com/parth05rohilla/bi-lstm-and-cnn-model-top-10/notebook\n",
    "- https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/code?competitionId=10025&searchQuery=cnn\n",
    "- https://colab.research.google.com/github/d2l-ai/d2l-en-colab/blob/master/chapter_natural-language-processing-applications/sentiment-analysis-cnn.ipynb\n",
    "- https://www.kaggle.com/nafisur/keras-models-lstm-cnn-gru-bidirectional-glove\n",
    "- https://www.kaggle.com/derrelldsouza/imdb-sentiment-analysis-eda-ml-lstm-bert#5.-Predictive-Modelling-using-Deep-Learning\n",
    "- https://www.kaggle.com/clementbrehard/imdb-conv1d-lstm"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 134715,
     "sourceId": 320111,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1786477,
     "sourceId": 2914524,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
