{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465638d6-cc0c-4c15-970f-1ef0e9c5862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#how many epochs?\n",
    "EPOCHS = 8\n",
    "\n",
    "#clean Tweets?\n",
    "CLEAN_TWEETS = False\n",
    "\n",
    "#use meta data?\n",
    "USE_META = True\n",
    "\n",
    "#add dense layer?\n",
    "ADD_DENSE = False\n",
    "DENSE_DIM = 64\n",
    "\n",
    "#add dropout?\n",
    "ADD_DROPOUT = True\n",
    "DROPOUT = .2\n",
    "\n",
    "#train BERT base model? \n",
    "TRAIN_BASE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6040f4-9b0f-412c-b153-129a228444c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the basics\n",
    "import os, re, math, string, pandas as pd, numpy as np, seaborn as sns\n",
    "\n",
    "#graphing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#deep learning\n",
    "import tensorflow as tf\n",
    "\n",
    "#nlp\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "#scaling\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2525ef8a-cae3-4e43-a475-1cc8347239e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97537127-dcef-4750-a088-be1b7828fc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30231, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5578c547-85b3-4183-ab5e-2b85a7760cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler()\n",
    "train_x, train_y = ros.fit_resample(np.array(df['text_clean']).reshape(-1, 1), np.array(df['sentiment']).reshape(-1, 1));\n",
    "train_os = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text_clean', 'sentiment']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a4f390-71d9-4948-877b-9024310cad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_os['text_clean'].values\n",
    "y = train_os['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d81434-02a4-48dc-8b3e-9f9bf1ac1e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48b0d3c8-e1dc-40b7-b427-93fe795a7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92d7554b-116b-49c8-9992-041b2b19574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c8d5e87-9d11-41be-8076-7b686f0510da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(data,maximum_len) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "  \n",
    "\n",
    "    for i in range(len(data)):\n",
    "        encoded = TOKENIZER.encode_plus(data[i],\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=maximum_len,\n",
    "                                        pad_to_max_length=True,\n",
    "                                        return_attention_mask=True)\n",
    "      \n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "        \n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0715cd6d-3c2c-4eb3-bfd2-451bfb244960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_layer, learning_rate, use_meta = USE_META, add_dense = ADD_DENSE,\n",
    "               dense_dim = DENSE_DIM, add_dropout = ADD_DROPOUT, dropout = DROPOUT):\n",
    "    \n",
    "    #define inputs\n",
    "    input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "    # meta_input = tf.keras.Input(shape = (meta_train.shape[1], ))\n",
    "    \n",
    "    #insert BERT layer\n",
    "    transformer_layer = model_layer([input_ids,attention_masks])\n",
    "    \n",
    "    #choose only last hidden-state\n",
    "    output = transformer_layer[1]\n",
    "    \n",
    "    #add meta data\n",
    "    # if use_meta:\n",
    "    #     output = tf.keras.layers.Concatenate()([output, meta_input])\n",
    "        \n",
    "    \n",
    "    #add dense relu layer\n",
    "    if add_dense:\n",
    "        print(\"Training with additional dense layer...\")\n",
    "        output = tf.keras.layers.Dense(dense_dim,activation='relu')(output)\n",
    "    \n",
    "    #add dropout\n",
    "    if add_dropout:\n",
    "        print(\"Training with dropout...\")\n",
    "        output = tf.keras.layers.Dropout(dropout)(output)\n",
    "    \n",
    "    #add final node for binary classification\n",
    "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "    \n",
    "    #assemble and compile\n",
    "    # if use_meta:\n",
    "    #     print(\"Training with meta-data...\")\n",
    "    #     model = tf.keras.models.Model(inputs = [input_ids,attention_masks, meta_input],outputs = output)\n",
    "    \n",
    "    # else:\n",
    "    print(\"Training without meta-data...\")\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "\n",
    "    model.compile(tf.keras.optimizers.Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78095b10-06f3-4687-8f08-464813db6433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding Tweets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/dgx914/.local/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets encoded\n",
      "\n",
      "Train length: 27336\n",
      "Test length: 3038\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_BASE:\n",
    "    #get our inputs\n",
    "    print('Encoding Tweets...')\n",
    "    train_input_ids,train_attention_masks = bert_encode(X_train,60)\n",
    "    test_input_ids,test_attention_masks = bert_encode(X_test,60)\n",
    "    print('Tweets encoded')\n",
    "    print('')\n",
    "\n",
    "    #debugging step\n",
    "    print('Train length:', len(train_input_ids))\n",
    "    print('Test length:', len(test_input_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4547cb0c-bdf5-4bbc-9544-5ce52199a986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#get BERT layer\n",
    "bert_large = TFBertModel.from_pretrained('bert-large-uncased')\n",
    "#bert_base = BertModel.from_pretrained('bert-large-uncased')          #to use with PyTorch\n",
    "\n",
    "#select BERT tokenizer\n",
    "TOKENIZER = BertTokenizer.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "973b4c12-c79e-4647-b040-22bc2d7881af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 27336\n",
      "Test length: 3038\n",
      "Training with dropout...\n",
      "Training without meta-data...\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 60)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     TFBaseModelOutputWit 335141888   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_73 (Dropout)            (None, 1024)         0           tf_bert_model[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        dropout_73[0][0]                 \n",
      "==================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/dgx914/.local/lib/python3.6/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 335,142,913\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#get our inputs\n",
    "train_input_ids,train_attention_masks = bert_encode(X_train,60)\n",
    "test_input_ids,test_attention_masks = bert_encode(X_test,60)\n",
    "\n",
    "#debugging step\n",
    "print('Train length:', len(train_input_ids))\n",
    "print('Test length:', len(test_input_ids))\n",
    "\n",
    "#and build and view parameters\n",
    "BERT_large = build_model(bert_large, learning_rate = 1e-5)\n",
    "BERT_large.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4c2386e-9165-4cd3-a8c4-9f332376086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('large_model.h5', monitor='val_loss', save_best_only = True, save_weights_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9016d95-f97f-4a61-b568-8fcca16c131a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "684/684 [==============================] - 4814s 7s/step - loss: 0.5613 - accuracy: 0.7064 - val_loss: 0.4896 - val_accuracy: 0.7580\n",
      "Epoch 2/8\n",
      "684/684 [==============================] - 5364s 8s/step - loss: 0.4630 - accuracy: 0.7789 - val_loss: 0.4919 - val_accuracy: 0.7652\n",
      "Epoch 3/8\n",
      "684/684 [==============================] - 5798s 8s/step - loss: 0.3824 - accuracy: 0.8296 - val_loss: 0.5138 - val_accuracy: 0.7710\n",
      "Epoch 4/8\n",
      "518/684 [=====================>........] - ETA: 17:11 - loss: 0.2659 - accuracy: 0.8883"
     ]
    }
   ],
   "source": [
    "#train BERT\n",
    "history = BERT_large.fit([train_input_ids,train_attention_masks], y_train, validation_split = .2, epochs = EPOCHS, callbacks = [checkpoint], batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88e0e5-dc16-4264-a856-348610b761a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de605e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
