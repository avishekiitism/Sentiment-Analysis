{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aeca3bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T06:54:05.002675Z",
     "iopub.status.busy": "2024-03-30T06:54:05.002203Z",
     "iopub.status.idle": "2024-03-30T06:54:26.763291Z",
     "shell.execute_reply": "2024-03-30T06:54:26.761989Z",
     "shell.execute_reply.started": "2024-03-30T06:54:05.002639Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [48 lines of output]\n",
      "  C:\\Users\\Avishek kumar\\AppData\\Local\\Temp\\pip-build-env-pxflqm64\\overlay\\Lib\\site-packages\\setuptools\\dist.py:318: InformationOnly: Normalizing '0.8.0.rc4' to '0.8.0rc4'\n",
      "    self.metadata.version = self._normalize_version(self.metadata.version)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  copying tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  copying tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  copying tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  copying tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  copying tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  copying tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-39\\tokenizers\\implementations\n",
      "  copying tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\n",
      "  copying tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\models\n",
      "  copying tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\decoders\n",
      "  copying tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\normalizers\n",
      "  copying tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\pre_tokenizers\n",
      "  copying tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\processors\n",
      "  copying tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-39\\tokenizers\\trainers\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached transformers-3.0.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (1.23.0)\n",
      "Collecting tokenizers==0.8.0-rc4 (from transformers==3)\n",
      "  Using cached tokenizers-0.8.0rc4.tar.gz (96 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: packaging in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (23.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (3.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==3) (2023.6.3)\n",
      "Collecting sentencepiece (from transformers==3)\n",
      "  Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting sacremoses (from transformers==3)\n",
      "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers==3) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==3) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==3) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==3) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==3) (2022.6.15)\n",
      "Requirement already satisfied: click in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacremoses->transformers==3) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sacremoses->transformers==3) (1.2.0)\n",
      "Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\n",
      "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Using cached sentencepiece-0.2.0-cp39-cp39-win_amd64.whl (991 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1ede1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T06:54:26.766328Z",
     "iopub.status.busy": "2024-03-30T06:54:26.765894Z",
     "iopub.status.idle": "2024-03-30T06:54:38.142631Z",
     "shell.execute_reply": "2024-03-30T06:54:38.141225Z",
     "shell.execute_reply.started": "2024-03-30T06:54:26.766289Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "c:\\users\\avishek kumar\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "# Torch ML libraries\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Misc.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ad3a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:52:41.236082Z",
     "iopub.status.busy": "2024-03-30T10:52:41.235618Z",
     "iopub.status.idle": "2024-03-30T10:52:41.443909Z",
     "shell.execute_reply": "2024-03-30T10:52:41.442417Z",
     "shell.execute_reply.started": "2024-03-30T10:52:41.236049Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('Corona_NLP_train.csv', encoding = 'latin-1')\n",
    "# df=df[:2000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8220b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:44.329019Z",
     "iopub.status.busy": "2024-03-30T10:56:44.328492Z",
     "iopub.status.idle": "2024-03-30T10:56:44.334489Z",
     "shell.execute_reply": "2024-03-30T10:56:44.333109Z",
     "shell.execute_reply.started": "2024-03-30T10:56:44.328982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the class balance.\n",
    "# sns.countplot(df.Sentiment)\n",
    "# plt.xlabel('review score');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468aa365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:45.412630Z",
     "iopub.status.busy": "2024-03-30T10:56:45.412157Z",
     "iopub.status.idle": "2024-03-30T10:56:45.427952Z",
     "shell.execute_reply": "2024-03-30T10:56:45.426829Z",
     "shell.execute_reply.started": "2024-03-30T10:56:45.412597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to convert score to sentiment\n",
    "def to_sentiment(rating):\n",
    "    \n",
    "    if(rating=='Neutral'):\n",
    "        return 2\n",
    "    elif rating=='Positive':\n",
    "        return 3\n",
    "    elif rating=='Extremely Positive':\n",
    "        return 4\n",
    "    elif rating=='Negative':\n",
    "        return 1\n",
    "    elif rating=='Extremely Negative':\n",
    "        return 0\n",
    "\n",
    "# Apply to the dataset \n",
    "df['sentiment'] = df.Sentiment.apply(to_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6d5aeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:47.959857Z",
     "iopub.status.busy": "2024-03-30T10:56:47.959396Z",
     "iopub.status.idle": "2024-03-30T10:56:47.966688Z",
     "shell.execute_reply": "2024-03-30T10:56:47.965207Z",
     "shell.execute_reply.started": "2024-03-30T10:56:47.959823Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution\n",
    "class_names = ['Extremely negative', 'Negative','Neutral', 'Positive','Extremely Positive']\n",
    "ax = sns.countplot(df.sentiment)\n",
    "plt.xlabel('review sentiment')\n",
    "ax.set_xticklabels(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b1057e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:48.355803Z",
     "iopub.status.busy": "2024-03-30T10:56:48.355373Z",
     "iopub.status.idle": "2024-03-30T10:56:48.576921Z",
     "shell.execute_reply": "2024-03-30T10:56:48.575496Z",
     "shell.execute_reply.started": "2024-03-30T10:56:48.355769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the model name\n",
    "MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# Build a BERT based tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189992a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:48.827920Z",
     "iopub.status.busy": "2024-03-30T10:56:48.827464Z",
     "iopub.status.idle": "2024-03-30T10:56:48.835902Z",
     "shell.execute_reply": "2024-03-30T10:56:48.834371Z",
     "shell.execute_reply.started": "2024-03-30T10:56:48.827887Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some of the common BERT tokens\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id) # marker for ending of a sentence\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id) # start of each sentence, so BERT knows we’re doing classification\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id) # special token for padding\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id) # tokens not found in training set \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62724d5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:50.446919Z",
     "iopub.status.busy": "2024-03-30T10:56:50.446490Z",
     "iopub.status.idle": "2024-03-30T10:56:53.437627Z",
     "shell.execute_reply": "2024-03-30T10:56:53.436235Z",
     "shell.execute_reply.started": "2024-03-30T10:56:50.446885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store length of each review \n",
    "token_lens = []\n",
    "\n",
    "# Iterate through the content slide\n",
    "for txt in df.OriginalTweet:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af388a62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:53.440554Z",
     "iopub.status.busy": "2024-03-30T10:56:53.440111Z",
     "iopub.status.idle": "2024-03-30T10:56:53.843321Z",
     "shell.execute_reply": "2024-03-30T10:56:53.841801Z",
     "shell.execute_reply.started": "2024-03-30T10:56:53.440519Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the distribution of review lengths \n",
    "sns.distplot(token_lens)\n",
    "plt.xlim([0, 256]);\n",
    "plt.xlabel('Token count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa0eeaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:55.048489Z",
     "iopub.status.busy": "2024-03-30T10:56:55.048077Z",
     "iopub.status.idle": "2024-03-30T10:56:55.054886Z",
     "shell.execute_reply": "2024-03-30T10:56:55.053431Z",
     "shell.execute_reply.started": "2024-03-30T10:56:55.048458Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 160\n",
    "RANDOM_SEED=42\n",
    "device='cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9836eab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:55.473764Z",
     "iopub.status.busy": "2024-03-30T10:56:55.473315Z",
     "iopub.status.idle": "2024-03-30T10:56:55.484428Z",
     "shell.execute_reply": "2024-03-30T10:56:55.482861Z",
     "shell.execute_reply.started": "2024-03-30T10:56:55.473732Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "    # Constructor Function \n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    # Length magic method\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    # get item magic method\n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "        \n",
    "        # Encoded format to be returned \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157d0cba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:56.161635Z",
     "iopub.status.busy": "2024-03-30T10:56:56.160239Z",
     "iopub.status.idle": "2024-03-30T10:56:56.174113Z",
     "shell.execute_reply": "2024-03-30T10:56:56.172799Z",
     "shell.execute_reply.started": "2024-03-30T10:56:56.161572Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "print(df_train.shape, df_val.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2a436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:56.886700Z",
     "iopub.status.busy": "2024-03-30T10:56:56.886216Z",
     "iopub.status.idle": "2024-03-30T10:56:56.894310Z",
     "shell.execute_reply": "2024-03-30T10:56:56.893020Z",
     "shell.execute_reply.started": "2024-03-30T10:56:56.886665Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        reviews=df.OriginalTweet.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6098a04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:57.660472Z",
     "iopub.status.busy": "2024-03-30T10:56:57.660051Z",
     "iopub.status.idle": "2024-03-30T10:56:57.669257Z",
     "shell.execute_reply": "2024-03-30T10:56:57.668084Z",
     "shell.execute_reply.started": "2024-03-30T10:56:57.660441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create train, test and val data loaders\n",
    "BATCH_SIZE = 16\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72eed5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:58.151239Z",
     "iopub.status.busy": "2024-03-30T10:56:58.150780Z",
     "iopub.status.idle": "2024-03-30T10:56:58.194874Z",
     "shell.execute_reply": "2024-03-30T10:56:58.193269Z",
     "shell.execute_reply.started": "2024-03-30T10:56:58.151207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Examples \n",
    "data = next(iter(train_data_loader))\n",
    "print(data.keys())\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c83a008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T10:56:58.453877Z",
     "iopub.status.busy": "2024-03-30T10:56:58.453467Z",
     "iopub.status.idle": "2024-03-30T10:56:59.132952Z",
     "shell.execute_reply": "2024-03-30T10:56:59.131864Z",
     "shell.execute_reply.started": "2024-03-30T10:56:58.453846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the basic BERT model \n",
    "bert_model = BertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605d482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T11:10:27.507327Z",
     "iopub.status.busy": "2024-03-30T11:10:27.506869Z",
     "iopub.status.idle": "2024-03-30T11:10:27.519635Z",
     "shell.execute_reply": "2024-03-30T11:10:27.518290Z",
     "shell.execute_reply.started": "2024-03-30T11:10:27.507295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build the Sentiment Classifier class \n",
    "class SentimentClassifier(nn.Module):\n",
    "    \n",
    "    # Constructor class \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.lstm = nn.LSTM(self.bert.config.hidden_size,\n",
    "                           hidden_size=384,\n",
    "                           num_layers=10,\n",
    "                           dropout=0.45,\n",
    "                           bidirectional=True)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "    \n",
    "    # Forward propagaion class\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "#         pooled_output=outputs[1]\n",
    "#         #  Add a dropout layer \n",
    "#         output = self.drop(pooled_output)\n",
    "#         return self.out(output)\n",
    "    \n",
    "        bert_output = outputs[0]  # BERT embeddings\n",
    "        \n",
    "        # Pass BERT embeddings through LSTM\n",
    "        lstm_output, _ = self.lstm(bert_output)\n",
    "        \n",
    "        # Apply dropout and pass through linear layer\n",
    "        lstm_output = self.drop(lstm_output)\n",
    "        logits = self.out(lstm_output[:, -1, :])  # Using only the last timestep output of LSTM\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fedef51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T11:10:27.726999Z",
     "iopub.status.busy": "2024-03-30T11:10:27.726574Z",
     "iopub.status.idle": "2024-03-30T11:10:28.471580Z",
     "shell.execute_reply": "2024-03-30T11:10:28.470459Z",
     "shell.execute_reply.started": "2024-03-30T11:10:27.726966Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the model and move to classifier\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb7e9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T11:10:28.474672Z",
     "iopub.status.busy": "2024-03-30T11:10:28.474184Z",
     "iopub.status.idle": "2024-03-30T11:10:28.480947Z",
     "shell.execute_reply": "2024-03-30T11:10:28.479810Z",
     "shell.execute_reply.started": "2024-03-30T11:10:28.474631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of hidden units\n",
    "print(bert_model.config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbed5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T11:10:28.482632Z",
     "iopub.status.busy": "2024-03-30T11:10:28.482262Z",
     "iopub.status.idle": "2024-03-30T11:10:28.497912Z",
     "shell.execute_reply": "2024-03-30T11:10:28.496698Z",
     "shell.execute_reply.started": "2024-03-30T11:10:28.482604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of iterations \n",
    "EPOCHS = 10\n",
    "\n",
    "# Optimizer Adam \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Set the loss function \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f233f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T11:10:28.500617Z",
     "iopub.status.busy": "2024-03-30T11:10:28.500224Z",
     "iopub.status.idle": "2024-03-30T11:10:28.514066Z",
     "shell.execute_reply": "2024-03-30T11:10:28.510948Z",
     "shell.execute_reply.started": "2024-03-30T11:10:28.500586Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for a single training iteration\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Backward prop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Descent\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49dd90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T11:10:28.516752Z",
     "iopub.status.busy": "2024-03-30T11:10:28.516326Z",
     "iopub.status.idle": "2024-03-30T11:10:28.530975Z",
     "shell.execute_reply": "2024-03-30T11:10:28.529817Z",
     "shell.execute_reply.started": "2024-03-30T11:10:28.516719Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Function for a single training iteration with tqdm\n",
    "def train_epoch_with_progress_bar(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    # Initialize tqdm\n",
    "    with tqdm(total=len(data_loader), desc=\"Training\") as pbar:\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Backward prop\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient Descent\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Update tqdm progress bar\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31b93e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T11:10:28.695606Z",
     "iopub.status.busy": "2024-03-30T11:10:28.695133Z",
     "iopub.status.idle": "2024-03-30T11:10:28.705641Z",
     "shell.execute_reply": "2024-03-30T11:10:28.704040Z",
     "shell.execute_reply.started": "2024-03-30T11:10:28.695573Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "            \n",
    "            # Get model ouptuts\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9578338c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-30T11:10:28.898556Z",
     "iopub.status.busy": "2024-03-30T11:10:28.898080Z",
     "iopub.status.idle": "2024-03-30T11:44:00.218221Z",
     "shell.execute_reply": "2024-03-30T11:44:00.215369Z",
     "shell.execute_reply.started": "2024-03-30T11:10:28.898516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [32:16<00:00, 19.36s/it, loss=1.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 1.586308504343033 accuracy 0.25375\n",
      "Val   loss 1.5904046297073364 accuracy 0.22\n",
      "\n",
      "CPU times: user 1h 5min, sys: 1min 42s, total: 1h 6min 43s\n",
      "Wall time: 33min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Show details \n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 10)\n",
    "    \n",
    "    train_acc, train_loss = train_epoch_with_progress_bar(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    "    \n",
    "    print(f\"Train loss {train_loss} accuracy {train_acc}\")\n",
    "    \n",
    "    # Get model performance (accuracy and loss)\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    "    \n",
    "    print(f\"Val   loss {val_loss} accuracy {val_acc}\")\n",
    "    print()\n",
    "    \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # If we beat prev performance\n",
    "    if val_acc > best_accuracy:\n",
    "#         torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255915a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483630f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4339697,
     "sourceId": 7455460,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
